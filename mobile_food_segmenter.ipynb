{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gX5nDswTo1z2"
      },
      "source": [
        "# Infering Nutritional Information from Semantic Image Segmentation\n",
        "\n",
        "This notebook contains the implementation of the semantic image segmentation model that forms the backbone of Grocerly's nutritient scoring and gamification functionality.\n",
        "\n",
        "We use Google's `mobile-food-segmenter-v1` model for image segmentation and classification. This model classifies each image segment into one of 26 food groups (e.g. grains or leafy greens). It is trained on the Nutrition5k dataset to use RGB data. For each food group, a number of examples for speific foods are provided in `example_foods.csv`. These example foods are then used to query OpenFoodFacts API (`https://world.openfoodfacts.org/`) for nutritional information and Nova scores.\n",
        "\n",
        "Portion estimation is based on a list of standard serving sizes for the food groups. These serving sizes are used to scale the nutritional information obtained from OpenFoodFacts in order to gauge the overall calories and macronutrients in a meal.\n",
        "\n",
        "Finally, detected food groups and their respective Nova scores are used to make recommendations for healthier food swaps.\n",
        "\n",
        "***NOTE:*** **The final code block contains functionality for you to run our model and predictions only on your own submitted image if you wish.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoY0usprpEZm"
      },
      "source": [
        "## Setup and Structure\n",
        "This notebook provides an inference framework on the segmentation/classification model. It can be used with the Nutrition5k test dataset, or with some realistic data collected by our team members (and team members' friends/families) throughout the day, which can be downloaded here: https://drive.google.com/drive/folders/1EX0ZKHau61OdcsPHKiuA0JBIdK9nmoCO?usp=sharing\n",
        "\n",
        "*   `data_real` should hold the image test data\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download model and label maps:"
      ],
      "metadata": {
        "id": "VY5eccwWiN-A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T6r9lCSixGPz"
      },
      "outputs": [],
      "source": [
        "!pip install openfoodfacts\n",
        "!gdown --id '1BZbkFjxPduKQuajk2GvCn9REM_gTximA'\n",
        "!gdown --id '1GbHq_T3DFItDk30F7hJNORQdHncrTFBZ'\n",
        "!gdown --id '1U9fDxFk3ScdTEWFo7cabQftSAmjBr8vk'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mxar67GlBnY8"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "# Standard library imports\n",
        "import ast\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "import sys\n",
        "import tarfile\n",
        "\n",
        "# Related third-party imports\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from PIL import Image\n",
        "from scipy import stats\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from transformers import DPTForDepthEstimation, DPTFeatureExtractor\n",
        "\n",
        "# Local application/library specific imports\n",
        "import openfoodfacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R3CRLbiIBazt"
      },
      "source": [
        "## Constants\n",
        "\n",
        "To avoid unnecessary output, a `DEBUG`- flag is added. If you want more information about underlying decisions, it can be changed to `True` in order to get more output.\n",
        "\n",
        "***NOTE:*** The Openfoodfacts-API requires a user agent. If the API calls time out, it helps to change this agent to any other name."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMczF-xTAyxU"
      },
      "outputs": [],
      "source": [
        "DEBUG = False\n",
        "api = openfoodfacts.API(user_agent=\"someuser/something\")\n",
        "model_filename = '/content/mobile-food-segmenter.tar.gz'\n",
        "extracted_folder_path = 'extracted_model'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7DIA7DZJ9LE"
      },
      "source": [
        "The following function uses the Google model in order to compute a semantic segmentation mask over an image provided to it as a method argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vM4ub4AtBYig"
      },
      "outputs": [],
      "source": [
        "def computeSegmentationMask(filename):\n",
        "\n",
        "    if not os.path.exists(extracted_folder_path):\n",
        "        with tarfile.open(model_filename, 'r:gz') as tar:\n",
        "            tar.extractall(path=extracted_folder_path)\n",
        "        if DEBUG:\n",
        "          print(\"Model extracted\")\n",
        "\n",
        "    # Load the image\n",
        "    image_path = filename\n",
        "    image = tf.image.decode_image(tf.io.read_file(image_path))\n",
        "    image = tf.image.resize(image, [513, 513])\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    if DEBUG:\n",
        "      print(\"Image loaded\")\n",
        "\n",
        "    # Check if the image is 3-channel RGB\n",
        "    if image.shape[-1] != 3:\n",
        "        print(\"Make sure your image is RGB.\")\n",
        "\n",
        "    # Expand dimensions for batch\n",
        "    image_batch = tf.expand_dims(image, 0)\n",
        "\n",
        "    # Load the local model with specified output keys\n",
        "    m = hub.KerasLayer(extracted_folder_path, signature_outputs_as_dict=True)\n",
        "    if DEBUG:\n",
        "      print(\"Model loaded\")\n",
        "\n",
        "    # Use the model\n",
        "    results = m(image_batch)\n",
        "\n",
        "    if DEBUG:\n",
        "      print(\"Model used\")\n",
        "\n",
        "    segmentation_probs = results['food_group_segmenter:semantic_probabilities'][0]\n",
        "    segmentation_mask = results['food_group_segmenter:semantic_predictions'][0]\n",
        "\n",
        "    return segmentation_probs, segmentation_mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v77OVm2AKWBe"
      },
      "source": [
        "Next, we use a CSV reader to parse information about food groups, standard food group serving sizes, and example foods for each group."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRaCJXM5ivno"
      },
      "outputs": [],
      "source": [
        "def read_csv_to_dict(file_path, example_path):\n",
        "    data_dict = {}\n",
        "    serving_sizes = {}\n",
        "    example_foods = {}\n",
        "\n",
        "    with open(file_path, 'r') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader)  # Skip the header row\n",
        "        for row in csvreader:\n",
        "            key, label, serving_size = row\n",
        "            data_dict[int(key)] = label.split('|')\n",
        "            serving_sizes[int(key)] = serving_size\n",
        "\n",
        "    with open(example_path, 'r') as csvfile:\n",
        "        csvreader = csv.reader(csvfile)\n",
        "        next(csvreader)  # Skip the header row\n",
        "        for row in csvreader:\n",
        "            key, ex1, ex2, ex3 = row\n",
        "            examples = [ex1, ex2, ex3]\n",
        "            example_foods[int(key)] = examples\n",
        "\n",
        "    return data_dict, serving_sizes, example_foods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKQ5Kn_VK053"
      },
      "source": [
        "Since the model tends to detect small irregularities in images and assign them to a food class, while Grocerly is focused on the overall composition of a meal, we only regard the 3 largest segmentation areas in an image (so long as they are not labelled as \"background\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQeXeq9qMzig"
      },
      "outputs": [],
      "source": [
        "def get_top_k_labels(segmentation_mask, k=3):\n",
        "    # Flatten the segmentation mask\n",
        "    flat_mask = segmentation_mask.flatten()\n",
        "    exclude_labels = [0] #, 23, 24, 25] # exclude background, food containers, cutlery\n",
        "\n",
        "    # Count the occurrence of each unique label in the mask\n",
        "    labels, counts = np.unique(flat_mask, return_counts=True)\n",
        "\n",
        "    # Apply exclusion mask to remove unwanted labels\n",
        "    exclusion_mask = np.isin(labels, exclude_labels, invert=True)\n",
        "    labels = labels[exclusion_mask]\n",
        "    counts = counts[exclusion_mask]\n",
        "\n",
        "    # Sort the counts in descending order to find the top labels\n",
        "    sorted_indices = np.argsort(-counts)\n",
        "    top_labels = labels[sorted_indices[:k]]\n",
        "    top_counts = counts[sorted_indices[:k]]\n",
        "\n",
        "    # Calculate the ratio of each label's count to the total count of the top labels\n",
        "    total_top_counts = np.sum(top_counts)\n",
        "    ratios = top_counts / total_top_counts\n",
        "\n",
        "    if DEBUG:\n",
        "      print(f\"Counting ratios: {ratios}\")\n",
        "    return top_labels, ratios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6Dw_GU6LHDX"
      },
      "source": [
        "Visualization for original images and segmentation masks:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGMpmyYfCp-d"
      },
      "outputs": [],
      "source": [
        "from scipy.ndimage import center_of_mass\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "def visualize_segmentation_mask(segmentation_mask, filename):\n",
        "    k = 3\n",
        "\n",
        "    # Load and preprocess the image as per the provided method\n",
        "    image_path = filename\n",
        "    image = tf.image.decode_image(tf.io.read_file(image_path))\n",
        "    image = tf.image.resize(image, [513, 513])\n",
        "    image = image / 255.0  # Normalize to [0, 1]\n",
        "\n",
        "    if DEBUG:\n",
        "      print(\"Image loaded\")\n",
        "\n",
        "    # Assuming segmentation_mask is a TensorFlow tensor, convert it to numpy\n",
        "    if isinstance(segmentation_mask, tf.Tensor):\n",
        "        segmentation_mask = segmentation_mask.numpy()\n",
        "\n",
        "    # Get the top k labels from the segmentation mask\n",
        "    top_k_labels = get_top_k_labels(segmentation_mask, k)\n",
        "\n",
        "    # Mask to only show top k labels in the segmentation\n",
        "    top_k_mask = np.isin(segmentation_mask, top_k_labels)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(10, 5))  # Adjusted for side-by-side view\n",
        "\n",
        "    # Display the original image on the left\n",
        "    ax[0].imshow(image)\n",
        "    ax[0].set_title('Original Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    # Display the segmented image on the right, filtered by top k labels\n",
        "    ax[1].imshow(image, alpha=0.1)\n",
        "    ax[1].imshow(top_k_mask * segmentation_mask, cmap='jet', alpha=0.8)\n",
        "    ax[1].set_title(f'Top {k} Food Groups')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    unique_segments = np.unique(segmentation_mask[top_k_mask])\n",
        "    for segment in unique_segments:\n",
        "        # Find the center of mass for each top k segment\n",
        "        centroid = center_of_mass(segmentation_mask == segment)\n",
        "\n",
        "        # Display the segment number at its centroid on the segmented image\n",
        "        ax[1].text(centroid[1], centroid[0], str(segment), color='white', ha='center', va='center')\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrPZBixlLVVx"
      },
      "source": [
        "Next, we query the API for the foods that are detected in an image. This returns nutritional information per 100g. We scale these values, based on the standard serving sizes defined in the label map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hm0Qo__YwzIV"
      },
      "outputs": [],
      "source": [
        "def fetch_data_from_api(query):\n",
        "\n",
        "    if DEBUG:\n",
        "      print(f\"Query term: {query}\")\n",
        "    query_data = api.product.text_search(query).get(\"products\")\n",
        "\n",
        "    calories, carbs, fat, protein, sugar, salt = extract_nutrient_data(query_data, query)\n",
        "    return calories, carbs, fat, protein, sugar, salt\n",
        "\n",
        "\n",
        "def extract_nutrient_data(api_obj, query_term):\n",
        "    allergens = api_obj[0].get(\"allergens\")\n",
        "    nutrients = api_obj[0].get(\"nutriments\")\n",
        "\n",
        "    json_data = {}\n",
        "    try:\n",
        "      json_data = {\n",
        "          \"name\": api_obj[0].get(\"abbreviated_product_name\"),\n",
        "          \"categories\": [],\n",
        "          \"nutrients\": {\n",
        "              \"nova-group\": nutrients[\"nova-group\"],\n",
        "              \"proteins_100g\": nutrients[\"proteins_100g\"],\n",
        "              \"saturated-fat_100g\": nutrients[\"saturated-fat_100g\"],\n",
        "              \"fat_100g\": nutrients[\"fat_100g\"],\n",
        "              \"energy_100g\": nutrients[\"energy_100g\"],\n",
        "              \"carbohydrates_100g\": nutrients[\"carbohydrates_100g\"]\n",
        "          },\n",
        "          \"allergens\": allergens\n",
        "      }\n",
        "    except Exception as e:\n",
        "      print(f\"Caught error: {e}\")\n",
        "      json_data[\"nutrients\"][\"nova-group\"] = 0\n",
        "      json_data[\"saturated-fat_100g\"] = 0\n",
        "\n",
        "    for i in range(1):\n",
        "        if i < len(api_obj[0].get(\"categories_hierarchy\")):\n",
        "            json_data[\"categories\"].append(api_obj[0].get(\"categories_hierarchy\")[i])\n",
        "\n",
        "    optional_nutrients = [\"salt_100g\", \"sugars_100g\", \"ph_100g\"]\n",
        "    for nutrient in optional_nutrients:\n",
        "        if nutrient in nutrients:\n",
        "            json_data[\"nutrients\"][nutrient] = nutrients[nutrient]\n",
        "\n",
        "    json_string = json.dumps(json_data)\n",
        "\n",
        "    if json_data[\"nutrients\"][\"nova-group\"] and int(json_data[\"nutrients\"][\"nova-group\"]) != 1:\n",
        "        recommend_healthy_suggestions(json_data, int(json_data[\"nutrients\"][\"nova-group\"]), 1)\n",
        "\n",
        "    # print(f\"API Response: {json_string}\")\n",
        "\n",
        "    nutrients = json_data.get('nutrients', {})\n",
        "    energy = nutrients.get('energy_100g', 0) # kJ\n",
        "    carbs = nutrients.get('carbohydrates_100g', 0)\n",
        "    fat = nutrients.get('fat_100g', 0)\n",
        "    protein = nutrients.get('proteins_100g', 0)\n",
        "    sugar = nutrients.get('sugars_100g', 0)\n",
        "    salt = nutrients.get('salt_100g', 0)\n",
        "\n",
        "    # print(calories, carbs, fat, protein)\n",
        "    calories = energy / 4.184\n",
        "    return calories, carbs, fat, protein, sugar, salt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a user enters (or photographs) food that is not ideal from a nutritional perspective - this is measured on the Nova score - the framework suggests a healthier alternative from the same food group but with a lower Nova score. Note that misclassifications can happen, these also affect the healthy swap suggestions."
      ],
      "metadata": {
        "id": "B72sjVGJjcw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recommend_healthy_suggestions(json_data, nova_group, num_suggestions):\n",
        "    \"\"\"\n",
        "        Recommends healthy food suggestions based on nutrient data.\n",
        "\n",
        "        Parameters:\n",
        "        - json_data (dict): JSON object containing nutrient data.\n",
        "        - nova_group (int): Nova group value for the product.\n",
        "        - num_suggestions (int): Number of healthy suggestions to recommend.\n",
        "\n",
        "        Returns:\n",
        "        None\n",
        "        \"\"\"\n",
        "    print(\"SWAP RECOMMENDATIONS (based on nova-group):\")\n",
        "\n",
        "    for category in json_data[\"categories\"]:\n",
        "        category = category.strip(\"en:\")\n",
        "        query_data = api.product.text_search(category).get(\"products\")\n",
        "        df = pd.DataFrame(query_data)\n",
        "        filtered_df = df[df[\"nutriments\"].apply(lambda x: \"nova-group\" in x and x[\"nova-group\"] < nova_group)]\n",
        "        item = min(num_suggestions, len(filtered_df))\n",
        "        for i in range(item):\n",
        "            data = filtered_df.iloc[i]\n",
        "            nutrients = data[\"nutriments\"]\n",
        "            recommendation = data[\"product_name\"]\n",
        "                # \"nutrients\": {\n",
        "                #     \"nova-group\": nutrients[\"nova-group\"],\n",
        "                #     \"proteins_100g\": nutrients[\"proteins_100g\"],\n",
        "                #     \"saturated-fat_100g\": nutrients[\"saturated-fat_100g\"],\n",
        "                #     \"fat_100g\": nutrients[\"fat_100g\"],\n",
        "                #     \"energy_100g\": nutrients[\"energy_100g\"],\n",
        "                #     \"carbohydrates_100g\": nutrients[\"carbohydrates_100g\"]\n",
        "                # },\n",
        "            print(recommendation)"
      ],
      "metadata": {
        "id": "02spl6xwja3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVlrjr8cLq2c"
      },
      "source": [
        "## Main Function\n",
        "Putting it all together: Finally we iterate over all images in the provided dataset and make predictions on the food groups present, as well as the nutritional values. Only run the following code block if you uploaded the dataset provided on Drive. Otherwise, skip ahead to the next code cell to try out only your own image. **If you did not upload the `data_real` folder, THIS CODE BLOCK WILL FAIL**, move on to the next one :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CuRbPdPKB8uT"
      },
      "outputs": [],
      "source": [
        "directory_path = '/content/data_real/'\n",
        "\n",
        "label_dict, serving_sizes, example_foods = read_csv_to_dict('/content/seefood_mobile_food_segmenter_V1_labelmap.csv', '/content/example_foods.csv')\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory_path):\n",
        "    if os.path.isfile(os.path.join(directory_path, filename)):\n",
        "      print(filename)\n",
        "\n",
        "      total_cals = []\n",
        "      total_protein = []\n",
        "      total_carbs = []\n",
        "      total_fat = []\n",
        "      total_salt = []\n",
        "      total_sugar = []\n",
        "      actual_ratios = []\n",
        "\n",
        "      with tqdm(total=100) as pbar:\n",
        "        class_probabilities, mask = computeSegmentationMask(os.path.join(directory_path, filename))\n",
        "        pbar.update(100)\n",
        "\n",
        "        visualize_segmentation_mask(mask, os.path.join(directory_path, filename))\n",
        "        top_labels, ratios = get_top_k_labels(mask.numpy(), k=3)\n",
        "\n",
        "        if DEBUG:\n",
        "          print(\"Top labels:\")\n",
        "\n",
        "        for i in range(0, len(top_labels)):\n",
        "            top_label  = top_labels[i]\n",
        "            ratio = ratios[i]\n",
        "            label_value = label_dict.get(top_label, \"Label not found\")  # Retrieve the value from label_dict or display a message if not found\n",
        "\n",
        "\n",
        "            mass_scale = int(serving_sizes[top_label]) / 100\n",
        "\n",
        "            if DEBUG:\n",
        "              print(f\"{label_value}\")\n",
        "\n",
        "            if top_label not in [23, 24, 25]:\n",
        "\n",
        "              # API-Call\n",
        "              actual_ratios.append(ratio)\n",
        "              calories, carbs, fat, protein, sugar, salt = fetch_data_from_api(np.random.choice(example_foods[top_label], 1))\n",
        "              total_cals.append(calories*mass_scale)\n",
        "              total_carbs.append(carbs*mass_scale)\n",
        "              total_fat.append(fat*mass_scale)\n",
        "              total_protein.append(protein*mass_scale)\n",
        "              total_sugar.append(sugar*mass_scale)\n",
        "              total_salt.append(salt*mass_scale)\n",
        "\n",
        "    if not np.isclose(np.sum(actual_ratios), 1):\n",
        "        # Scale each ratio by dividing by the total sum to make the sum of ratios equal to 1\n",
        "        ratios = [ratio / np.sum(actual_ratios) for ratio in actual_ratios]\n",
        "\n",
        "    if DEBUG:\n",
        "      print(f\"Ratios: {ratios}\")\n",
        "\n",
        "    # Compute weighted averages\n",
        "    weighted_avg_cals = np.dot(ratios, total_cals)\n",
        "    weighted_avg_carbs = np.dot(ratios, total_carbs)\n",
        "    weighted_avg_fat = np.dot(ratios, total_fat)\n",
        "    weighted_avg_protein = np.dot(ratios, total_protein)\n",
        "    weighted_avg_sugar = np.dot(ratios, total_sugar)\n",
        "    weighted_avg_salt = np.dot(ratios, total_salt)\n",
        "\n",
        "    print(f\"\"\"\n",
        "      Nutritional Information for the Meal:\n",
        "      ----------------------------------\n",
        "      Calories:      {weighted_avg_cals:.2f} kcal\n",
        "      Carbohydrates: {weighted_avg_carbs:.2f} g\n",
        "      Protein:       {weighted_avg_protein:.2f} g\n",
        "      Fat:           {weighted_avg_fat:.2f} g\n",
        "      Sugar:         {weighted_avg_sugar:.2f} g\n",
        "      Salt:          {weighted_avg_salt:.2f} g\n",
        "      \"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test your own image\n",
        "\n",
        "Upload an image in a .png or .jpeg format to the Colab workspace. Ideally, your picture is taken from above and in good, natural lighting. Right-click your image file and select `Copy path`. Enter this path as `filename` in the code block below, then run the cell.\n",
        "\n",
        "Gro likes to eat Bamboo...what about you?"
      ],
      "metadata": {
        "id": "c1YRyguJj1iu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_dict, serving_sizes, example_foods = read_csv_to_dict('/content/seefood_mobile_food_segmenter_V1_labelmap.csv', '/content/example_foods.csv')\n",
        "\n",
        "filename = '<your-file-name>'\n",
        "\n",
        "total_cals = []\n",
        "total_protein = []\n",
        "total_carbs = []\n",
        "total_fat = []\n",
        "total_salt = []\n",
        "total_sugar = []\n",
        "actual_ratios = []\n",
        "\n",
        "with tqdm(total=100, desc=\"Computing Nutritional Facts\") as pbar:\n",
        "  class_probabilities, mask = computeSegmentationMask(filename)\n",
        "  pbar.update(100)\n",
        "\n",
        "  visualize_segmentation_mask(mask, filename)\n",
        "  top_labels, ratios = get_top_k_labels(mask.numpy(), k=3)\n",
        "\n",
        "  if DEBUG:\n",
        "    print(\"Top labels:\")\n",
        "\n",
        "  for i in range(0, len(top_labels)):\n",
        "      top_label  = top_labels[i]\n",
        "      ratio = ratios[i]\n",
        "      label_value = label_dict.get(top_label, \"Label not found\")  # Retrieve the value from label_dict or display a message if not found\n",
        "\n",
        "\n",
        "      mass_scale = int(serving_sizes[top_label]) / 100\n",
        "\n",
        "      if DEBUG:\n",
        "        print(f\"{label_value}\")\n",
        "\n",
        "      if top_label not in [23, 24, 25]:\n",
        "\n",
        "        # API-Call\n",
        "        actual_ratios.append(ratio)\n",
        "        calories, carbs, fat, protein, sugar, salt = fetch_data_from_api(np.random.choice(example_foods[top_label], 1))\n",
        "        total_cals.append(calories*mass_scale)\n",
        "        total_carbs.append(carbs*mass_scale)\n",
        "        total_fat.append(fat*mass_scale)\n",
        "        total_protein.append(protein*mass_scale)\n",
        "        total_sugar.append(sugar*mass_scale)\n",
        "        total_salt.append(salt*mass_scale)\n",
        "\n",
        "if not np.isclose(np.sum(actual_ratios), 1):\n",
        "    # Scale each ratio by dividing by the total sum to make the sum of ratios equal to 1\n",
        "    ratios = [ratio / np.sum(actual_ratios) for ratio in actual_ratios]\n",
        "\n",
        "if DEBUG:\n",
        "  print(f\"Ratios: {ratios}\")\n",
        "\n",
        "# Compute weighted averages\n",
        "weighted_avg_cals = np.dot(ratios, total_cals)\n",
        "weighted_avg_carbs = np.dot(ratios, total_carbs)\n",
        "weighted_avg_fat = np.dot(ratios, total_fat)\n",
        "weighted_avg_protein = np.dot(ratios, total_protein)\n",
        "weighted_avg_sugar = np.dot(ratios, total_sugar)\n",
        "weighted_avg_salt = np.dot(ratios, total_salt)\n",
        "\n",
        "print(f\"\"\"\n",
        "  Nutritional Information for the Meal:\n",
        "  ----------------------------------\n",
        "  Calories:      {weighted_avg_cals:.2f} kcal\n",
        "  Carbohydrates: {weighted_avg_carbs:.2f} g\n",
        "  Protein:       {weighted_avg_protein:.2f} g\n",
        "  Fat:           {weighted_avg_fat:.2f} g\n",
        "  Sugar:         {weighted_avg_sugar:.2f} g\n",
        "  Salt:          {weighted_avg_salt:.2f} g\n",
        "  \"\"\")\n"
      ],
      "metadata": {
        "id": "DEnQz0h1ip7Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations and Future Work\n",
        "\n",
        "The model is pre-trained and includes only 26 food groups - not actual foods or ingredients. This means that a number of assumptions were made, for example about standard portion sizes. Furthermore, we only provide 3 example foods/ingredients per food group with which to query the nutrient fact API. Within the time and resource limits of the development process, some reduction in accuracy was necessary. Unfortunately, this leads to some interesting misclassifications at times.\n",
        "\n",
        "In future development, we would like to re-train this model to recognize a higher number of ingredient classes. This would require additional segmentation and annotation of a test set. Additionally, in future iterations of the app, we would dynamically estimate portion sizes, either by adding a learning algorithm on top of the existing model or by leveraging higher classification accuracy in order to better estimate the proportional amount of food groups in a picture."
      ],
      "metadata": {
        "id": "Gvxg5vPxkVID"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}